{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vFnu0XZUrjas"
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import numpy as np\n",
    "import tifffile as tiff\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import CSVLogger\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.callbacks import CSVLogger\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Rvb_3GXHwsHd",
    "outputId": "07c93aca-04e8-478e-9e8c-911401521fba"
   },
   "outputs": [],
   "source": [
    "# u-net model with up-convolution or up-sampling and weighted binary-crossentropy as loss func\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Conv2DTranspose, BatchNormalization, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import plot_model\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "def unet_model(n_classes=5, im_sz=160, n_channels=8, n_filters_start=32, growth_factor=2, upconv=True,\n",
    "               class_weights=[0.2, 0.3, 0.1, 0.1, 0.3]):\n",
    "    droprate=0.25\n",
    "    n_filters = n_filters_start\n",
    "    inputs = Input((im_sz, im_sz, n_channels))\n",
    "    #inputs = BatchNormalization()(inputs)\n",
    "    conv1 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    conv1 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    #pool1 = Dropout(droprate)(pool1)\n",
    "\n",
    "    n_filters *= growth_factor\n",
    "    pool1 = BatchNormalization()(pool1)\n",
    "    conv2 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(pool1)\n",
    "    conv2 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    pool2 = Dropout(droprate)(pool2)\n",
    "\n",
    "    n_filters *= growth_factor\n",
    "    pool2 = BatchNormalization()(pool2)\n",
    "    conv3 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(pool2)\n",
    "    conv3 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    pool3 = Dropout(droprate)(pool3)\n",
    "\n",
    "    n_filters *= growth_factor\n",
    "    pool3 = BatchNormalization()(pool3)\n",
    "    conv4_0 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(pool3)\n",
    "    conv4_0 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv4_0)\n",
    "    pool4_1 = MaxPooling2D(pool_size=(2, 2))(conv4_0)\n",
    "    pool4_1 = Dropout(droprate)(pool4_1)\n",
    "\n",
    "    n_filters *= growth_factor\n",
    "    pool4_1 = BatchNormalization()(pool4_1)\n",
    "    conv4_1 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(pool4_1)\n",
    "    conv4_1 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv4_1)\n",
    "    pool4_2 = MaxPooling2D(pool_size=(2, 2))(conv4_1)\n",
    "    pool4_2 = Dropout(droprate)(pool4_2)\n",
    "\n",
    "    n_filters *= growth_factor\n",
    "    conv5 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(pool4_2)\n",
    "    conv5 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv5)\n",
    "\n",
    "    n_filters //= growth_factor\n",
    "    if upconv:\n",
    "        up6_1 = concatenate([Conv2DTranspose(n_filters, (2, 2), strides=(2, 2), padding='same')(conv5), conv4_1])\n",
    "    else:\n",
    "        up6_1 = concatenate([UpSampling2D(size=(2, 2))(conv5), conv4_1])\n",
    "    up6_1 = BatchNormalization()(up6_1)\n",
    "    conv6_1 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(up6_1)\n",
    "    conv6_1 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv6_1)\n",
    "    conv6_1 = Dropout(droprate)(conv6_1)\n",
    "\n",
    "    n_filters //= growth_factor\n",
    "    if upconv:\n",
    "        up6_2 = concatenate([Conv2DTranspose(n_filters, (2, 2), strides=(2, 2), padding='same')(conv6_1), conv4_0])\n",
    "    else:\n",
    "        up6_2 = concatenate([UpSampling2D(size=(2, 2))(conv6_1), conv4_0])\n",
    "    up6_2 = BatchNormalization()(up6_2)\n",
    "    conv6_2 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(up6_2)\n",
    "    conv6_2 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv6_2)\n",
    "    conv6_2 = Dropout(droprate)(conv6_2)\n",
    "\n",
    "    n_filters //= growth_factor\n",
    "    if upconv:\n",
    "        up7 = concatenate([Conv2DTranspose(n_filters, (2, 2), strides=(2, 2), padding='same')(conv6_2), conv3])\n",
    "    else:\n",
    "        up7 = concatenate([UpSampling2D(size=(2, 2))(conv6_2), conv3])\n",
    "    up7 = BatchNormalization()(up7)\n",
    "    conv7 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(up7)\n",
    "    conv7 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv7)\n",
    "    conv7 = Dropout(droprate)(conv7)\n",
    "\n",
    "    n_filters //= growth_factor\n",
    "    if upconv:\n",
    "        up8 = concatenate([Conv2DTranspose(n_filters, (2, 2), strides=(2, 2), padding='same')(conv7), conv2])\n",
    "    else:\n",
    "        up8 = concatenate([UpSampling2D(size=(2, 2))(conv7), conv2])\n",
    "    up8 = BatchNormalization()(up8)\n",
    "    conv8 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(up8)\n",
    "    conv8 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv8)\n",
    "    conv8 = Dropout(droprate)(conv8)\n",
    "\n",
    "    n_filters //= growth_factor\n",
    "    if upconv:\n",
    "        up9 = concatenate([Conv2DTranspose(n_filters, (2, 2), strides=(2, 2), padding='same')(conv8), conv1])\n",
    "    else:\n",
    "        up9 = concatenate([UpSampling2D(size=(2, 2))(conv8), conv1])\n",
    "    conv9 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(up9)\n",
    "    conv9 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv9)\n",
    "\n",
    "    conv10 = Conv2D(n_classes, (1, 1), activation='sigmoid')(conv9)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=conv10)\n",
    "\n",
    "    def weighted_binary_crossentropy(y_true, y_pred):\n",
    "        class_loglosses = K.mean(K.binary_crossentropy(y_true, y_pred), axis=[0, 1, 2])\n",
    "        return K.sum(class_loglosses * K.constant(class_weights))\n",
    "\n",
    "    model.compile(optimizer=Adam(), loss=weighted_binary_crossentropy)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gsHneiqBwn9T"
   },
   "outputs": [],
   "source": [
    "def get_rand_patch(img, mask, sz=160):\n",
    "    \"\"\"\n",
    "    :param img: ndarray with shape (x_sz, y_sz, num_channels)\n",
    "    :param mask: binary ndarray with shape (x_sz, y_sz, num_classes)\n",
    "    :param sz: size of random patch\n",
    "    :return: patch with shape (sz, sz, num_channels)\n",
    "    \"\"\"\n",
    "    assert len(img.shape) == 3 and img.shape[0] > sz and img.shape[1] > sz and img.shape[0:2] == mask.shape[0:2]\n",
    "    xc = random.randint(0, img.shape[0] - sz)\n",
    "    yc = random.randint(0, img.shape[1] - sz)\n",
    "    patch_img = img[xc:(xc + sz), yc:(yc + sz)]\n",
    "    patch_mask = mask[xc:(xc + sz), yc:(yc + sz)]\n",
    "\n",
    "    # Apply some random transformations\n",
    "    random_transformation = np.random.randint(1,8)\n",
    "    if random_transformation == 1:  # reverse first dimension\n",
    "        patch_img = patch_img[::-1,:,:]\n",
    "        patch_mask = patch_mask[::-1,:,:]\n",
    "    elif random_transformation == 2:    # reverse second dimension\n",
    "        patch_img = patch_img[:,::-1,:]\n",
    "        patch_mask = patch_mask[:,::-1,:]\n",
    "    elif random_transformation == 3:    # transpose(interchange) first and second dimensions\n",
    "        patch_img = patch_img.transpose([1,0,2])\n",
    "        patch_mask = patch_mask.transpose([1,0,2])\n",
    "    elif random_transformation == 4:\n",
    "        patch_img = np.rot90(patch_img, 1)\n",
    "        patch_mask = np.rot90(patch_mask, 1)\n",
    "    elif random_transformation == 5:\n",
    "        patch_img = np.rot90(patch_img, 2)\n",
    "        patch_mask = np.rot90(patch_mask, 2)\n",
    "    elif random_transformation == 6:\n",
    "        patch_img = np.rot90(patch_img, 3)\n",
    "        patch_mask = np.rot90(patch_mask, 3)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    return patch_img, patch_mask\n",
    "\n",
    "\n",
    "def get_patches(x_dict, y_dict, n_patches, sz=160):\n",
    "    x = list()\n",
    "    y = list()\n",
    "    total_patches = 0\n",
    "    while total_patches < n_patches:\n",
    "        img_id = random.sample(x_dict.keys(), 1)[0]\n",
    "        img = x_dict[img_id]\n",
    "        mask = y_dict[img_id]\n",
    "        img_patch, mask_patch = get_rand_patch(img, mask, sz)\n",
    "        x.append(img_patch)\n",
    "        y.append(mask_patch)\n",
    "        total_patches += 1\n",
    "    print('Generated {} patches'.format(total_patches))\n",
    "    return np.array(x), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oAReAOi2sXfW"
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "N_BANDS = 4\n",
    "N_CLASSES = 8  # Roads, Trees, Bare Soil, Rails, Buildings, Grass, Water, Pools\n",
    "CLASS_WEIGHTS = [0.2, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
    "N_EPOCHS = 150\n",
    "UPCONV = True\n",
    "PATCH_SZ = 160   # should divide by 16\n",
    "BATCH_SIZE = 150\n",
    "TRAIN_SZ = 4000  # train size\n",
    "VAL_SZ = 1000    # validation size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aePGwiUUtInH"
   },
   "outputs": [],
   "source": [
    "def normalize(img):\n",
    "    min = img.min()\n",
    "    max = img.max()\n",
    "    x = 2.0 * (img - min) / (max - min) - 1.0\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6BWzgbrPtKWB"
   },
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    return unet_model(N_CLASSES, PATCH_SZ, n_channels=N_BANDS, upconv=UPCONV, class_weights=CLASS_WEIGHTS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Viy3YhSntM0Q"
   },
   "outputs": [],
   "source": [
    "weights_path = 'weights'\n",
    "if not os.path.exists(weights_path):\n",
    "    os.makedirs(weights_path)\n",
    "weights_path += '/unet_weights.hdf5'\n",
    "\n",
    "trainIds = [str(i) for i in range(1, 15)]  # all availiable ids: from \"01\" to \"24\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cH2PpsgZtQMa"
   },
   "outputs": [],
   "source": [
    "X_DICT_TRAIN = dict()\n",
    "Y_DICT_TRAIN = dict()\n",
    "X_DICT_VALIDATION = dict()\n",
    "Y_DICT_VALIDATION = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 640
    },
    "colab_type": "code",
    "id": "AjUhlakWx_pe",
    "outputId": "13deab4a-2fc5-414e-e0ed-bb2183997467"
   },
   "outputs": [],
   "source": [
    "def loadImages():\n",
    "    print('Reading images')\n",
    "    for img_id in trainIds:\n",
    "        img_m = normalize(tiff.imread('./data/sat/{}.tif'.format(img_id)).transpose([1, 2, 0]))\n",
    "        mask = tiff.imread('./data/gt/{}.tif'.format(img_id)).transpose([1, 2, 0]) / 255\n",
    "        train_xsz = int(3/4 * img_m.shape[0])  # use 75% of image as train and 25% for validation\n",
    "        X_DICT_TRAIN[img_id] = img_m[:train_xsz, :, :]\n",
    "        Y_DICT_TRAIN[img_id] = mask[:train_xsz, :, :]\n",
    "        X_DICT_VALIDATION[img_id] = img_m[train_xsz:, :, :]\n",
    "        Y_DICT_VALIDATION[img_id] = mask[train_xsz:, :, :]\n",
    "        print(img_id + ' read')\n",
    "    print('Images were read')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading images\n",
      "1 read\n",
      "2 read\n",
      "3 read\n",
      "4 read\n",
      "5 read\n",
      "6 read\n",
      "7 read\n",
      "8 read\n",
      "9 read\n",
      "10 read\n",
      "11 read\n",
      "12 read\n",
      "13 read\n",
      "14 read\n",
      "Images were read\n"
     ]
    }
   ],
   "source": [
    "loadImages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_DICT_TRAIN.keys())\n",
    "print(x_val)\n",
    "print(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wxy067o1x01F"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start train net\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-d50a0c977e89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# def train_net():\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"start train net\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_patches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_DICT_TRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_DICT_TRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_patches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTRAIN_SZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPATCH_SZ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_patches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_DICT_VALIDATION\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_DICT_VALIDATION\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_patches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mVAL_SZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPATCH_SZ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-4528765d63a6>\u001b[0m in \u001b[0;36mget_patches\u001b[0;34m(x_dict, y_dict, n_patches, sz)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimg_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimg_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mimg_patch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_patch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_rand_patch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_patch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_patch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-4528765d63a6>\u001b[0m in \u001b[0;36mget_rand_patch\u001b[0;34m(img, mask, sz)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpatch\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \"\"\"\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0msz\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0msz\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mxc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0msz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0myc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0msz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# def train_net():\n",
    "print(\"start train net\")\n",
    "x_train, y_train = get_patches(X_DICT_TRAIN, Y_DICT_TRAIN, n_patches=TRAIN_SZ, sz=PATCH_SZ)\n",
    "x_val, y_val = get_patches(X_DICT_VALIDATION, Y_DICT_VALIDATION, n_patches=VAL_SZ, sz=PATCH_SZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "if os.path.isfile(weights_path):\n",
    "    model.load_weights(weights_path)\n",
    "#model_checkpoint = ModelCheckpoint(weights_path, monitor='val_loss', save_weights_only=True, save_best_only=True)\n",
    "#early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='auto')\n",
    "#reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.1, patience=5, min_lr=0.00001)\n",
    "model_checkpoint = ModelCheckpoint(weights_path, monitor='val_loss', save_best_only=True)\n",
    "csv_logger = CSVLogger('log_unet.csv', append=True, separator=';')\n",
    "tensorboard = TensorBoard(log_dir='./tensorboard_unet/', write_graph=True, write_images=True)\n",
    "model.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=N_EPOCHS,\n",
    "          verbose=2, shuffle=True,\n",
    "          callbacks=[model_checkpoint, \n",
    "                     csv_logger, \n",
    "                     tensorboard],\n",
    "          validation_data=(x_val, y_val))\n",
    "return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadImages()\n",
    "model = train_net()\n",
    "\n",
    "# evaluate the model\n",
    "scores_train = model.evaluate(x_train, y_train, verbose=0)\n",
    "scores_val = model.evaluate(x_val, y_val, verbose=0)\n",
    "# print(\"Training %s: %.2f%%\" % (model.metrics_names[1], scores_train[1]*100))\n",
    "print(\"Validation %s: %.2f%%\" % (model.metrics_names[1], scores_val[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "test.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
